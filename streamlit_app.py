import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
import time
import hashlib
import json
from pathlib import Path
from typing import Optional, List, Dict, Any
import fnmatch

# Load environment variables
load_dotenv()

# Set page configuration FIRST
st.set_page_config(
    layout="wide", 
    page_title="Advanced RAG Chatbot",
    page_icon="ü§ñ",
    initial_sidebar_state="expanded"
)

# Import dependencies with proper error handling
@st.cache_data
def check_dependencies():
    """Check if all required packages are available with correct versions"""
    missing_packages = []
    try:
        # Updated imports for latest versions
        import google.generativeai as genai
        from langchain_google_genai import ChatGoogleGenerativeAI
        from langchain_huggingface import HuggingFaceEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain_community.document_loaders import PyPDFLoader, CSVLoader, UnstructuredExcelLoader
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from langchain.memory import ConversationBufferMemory
        from langchain.chains import ConversationalRetrievalChain
    except ImportError as e:
        missing_package = str(e).split("'")[1] if "'" in str(e) else str(e)
        missing_packages.append(missing_package)
    
    return missing_packages

# Check dependencies early
missing_deps = check_dependencies()
if missing_deps:
    st.error(f"Missing packages: {', '.join(missing_deps)}")
    st.info("""
    Please install with:
    ```bash
    pip install -r requirements.txt
    ```
    """)
    st.stop()

# Import all required packages
try:
    import google.generativeai as genai
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_community.document_loaders import (
        PyPDFLoader, 
        CSVLoader, 
        TextLoader,
        UnstructuredExcelLoader
    )
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain.memory import ConversationBufferMemory
    from langchain.chains import ConversationalRetrievalChain
    from langchain.schema import Document
    import pandas as pd
    import numpy as np
    
except ImportError as e:
    st.error(f"Import error: {e}")
    st.info("Please check your package versions and requirements.txt")
    st.stop()

# Configuration
GOOGLE_API_KEY = (
    st.secrets.get("GOOGLE_API_KEY") or 
    st.secrets.get("GEMINI_API_KEY") or 
    os.getenv("GOOGLE_API_KEY") or 
    os.getenv("GEMINI_API_KEY")
)

if not GOOGLE_API_KEY:
    st.error("üö® Google API Key not found!")
    st.info("""
    **Setup Google API Key:**
    1. Go to https://makersuite.google.com/app/apikey
    2. Create API key
    3. Add to Streamlit secrets as GOOGLE_API_KEY
    """)
    st.stop()

# Configure Google AI
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
genai.configure(api_key=GOOGLE_API_KEY)

# Setup persistent storage
CACHE_DIR = Path("./vector_cache")
CACHE_DIR.mkdir(exist_ok=True)

# Global app state file to track initialization across users
APP_STATE_FILE = Path("./app_state.json")

def get_app_state():
    """Get global app state"""
    try:
        if APP_STATE_FILE.exists():
            with open(APP_STATE_FILE, 'r') as f:
                return json.load(f)
    except:
        pass
    return {"initialized": False, "last_load": 0}

def save_app_state(state):
    """Save global app state"""
    try:
        with open(APP_STATE_FILE, 'w') as f:
            json.dump(state, f)
    except:
        pass

# Translations
translations = {
    "en": {
        "title": "ü§ñ Gen AI : RAG Chatbot with Documents (Demo)",
        "upload_button": "Upload Additional Documents",
        "ask_placeholder": "Ask a question in Thai or English...",
        "processing": "Processing documents...",
        "welcome": "üëã Hello! I'm ready to chat about various topics based on the documents.",
        "upload_success": lambda count: f"‚úÖ {count} document(s) uploaded successfully!",
        "thinking": "üß† Generating response...",
        "language": "Language / ‡∏†‡∏≤‡∏©‡∏≤",
        "clear_chat": "üóëÔ∏è Clear Chat",
        "clear_cache": "üóëÔ∏è Clear Cache",
        "reload_local": "üîÑ Reload Local Files",
        "model_info": '<span class="emoji">ü§ñ</span><span class="bold-text">Model:</span> Gemini Pro | <span class="emoji">üìä</span><span class="bold-text">Embedding:</span> MiniLM-L6-v2 | <span class="emoji">üóÉÔ∏è</span><span class="bold-text">Vector DB:</span> FAISS',
        "no_documents": "üìÑ No documents found. Please check the repository or upload files.",
        "error_processing": "‚ùå Error processing documents. Please try again.",
        "error_response": "üö® Sorry, I encountered an error while generating response.",
        "checking_cache": "Checking cache...",
        "found_cached": "Found cached vectors",
        "saving_cache": "Saving to cache...",
        "local_files": "üìÅ Local Repository Files",
        "uploaded_files": "üì§ Uploaded Files",
        "stats": "Statistics",
        "advanced_features": "Advanced Features",
        "auto_loaded": "Auto-loaded from repository",
        "processing_local": "Processing repository files...",
        "found_local_files": lambda count: f"Found {count} local files in repository",
        "system_ready": "System initialized and ready!",
        "loading_complete": "Loading complete",
    },
    "th": {
        "title": "ü§ñ Gen AI : RAG Chatbot with Documents (Demo)",
        "upload_button": "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°",
        "ask_placeholder": "‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©...",
        "processing": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£...",
        "welcome": "üëã ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ! ‡∏â‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ",
        "upload_success": lambda count: f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ {count} ‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!",
        "thinking": "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...",
        "language": "‡∏†‡∏≤‡∏©‡∏≤ / Language",
        "clear_chat": "üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó",
        "clear_cache": "üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á Cache",
        "reload_local": "üîÑ ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå local ‡πÉ‡∏´‡∏°‡πà",
        "model_info": '<span class="emoji">ü§ñ</span><span class="bold-text">‡πÇ‡∏°‡πÄ‡∏î‡∏•:</span> Gemini Pro | <span class="emoji">üìä</span><span class="bold-text">Embedding:</span> MiniLM-L6-v2 | <span class="emoji">üóÉÔ∏è</span><span class="bold-text">Vector DB:</span> FAISS',
        "no_documents": "üìÑ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö repository ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå",
        "error_processing": "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
        "error_response": "üö® ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö",
        "checking_cache": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö cache...",
        "found_cached": "‡∏û‡∏ö vectors ‡πÉ‡∏ô cache",
        "saving_cache": "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á cache...",
        "local_files": "üìÅ ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Repository",
        "uploaded_files": "üì§ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î",
        "stats": "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥",
        "advanced_features": "‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á",
        "auto_loaded": "‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å repository",
        "processing_local": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô repository...",
        "found_local_files": lambda count: f"‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå local {count} ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô repository",
        "system_ready": "‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!",
        "loading_complete": "‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô",
    }
}

# Initialize session state
def init_session_state():
    defaults = {
        "messages": [],
        "vectorstore": None,
        "language": "en",
        "uploaded_files": [],
        "local_files": [],
        "documents_processed": False,
        "document_chunks": 0,
        "debug_mode": False,
        "last_request_time": 0,
        "app_initialized": False,
        "file_analysis": {},
        "search_history": [],
        "similarity_threshold": 0.7,
        "max_tokens": 512,
        "temperature": 0.1,
        "auto_load_attempted": False,
        "show_loading_messages": True,
        "initialization_complete": False,
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

init_session_state()

# Function to scan for local files
def load_gitignore_patterns():
    """Load .gitignore patterns to exclude files"""
    patterns = []
    try:
        if os.path.exists('.gitignore'):
            with open('.gitignore', 'r', encoding='utf-8') as f:
                patterns = [line.strip() for line in f.readlines() 
                           if line.strip() and not line.startswith('#')]
    except:
        pass
    
    # Default patterns to ignore
    default_patterns = [
        '.git/*', '*.pyc', '__pycache__/*', '.env', '*.log',
        'node_modules/*', '.vscode/*', '.idea/*', '*.tmp',
        'vector_cache/*', '.streamlit/*'
    ]
    
    return patterns + default_patterns

def should_ignore_file(filepath, patterns):
    """Check if file should be ignored based on patterns"""
    filename = os.path.basename(filepath)
    
    # Always ignore these
    if filename.startswith('.') or filename == 'requirements.txt':
        return True
    
    for pattern in patterns:
        if fnmatch.fnmatch(filepath, pattern) or fnmatch.fnmatch(filename, pattern):
            return True
    
    return False

@st.cache_data
def scan_local_files():
    """Scan repository for document files"""
    supported_extensions = ('.pdf', '.txt', '.csv', '.xlsx', '.xls', '.docx')
    local_files = []
    ignore_patterns = load_gitignore_patterns()
    
    try:
        # Scan current directory and subdirectories
        for root, dirs, files in os.walk('.'):
            # Skip hidden directories and common ignore directories
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules', 'vector_cache']]
            
            for file in files:
                if file.lower().endswith(supported_extensions):
                    filepath = os.path.join(root, file)
                    if not should_ignore_file(filepath, ignore_patterns):
                        local_files.append(filepath)
        
        # Remove duplicates and sort
        local_files = sorted(list(set(local_files)))
        
    except Exception as e:
        pass
    
    return local_files

# Optimized embeddings with latest HuggingFace integration
@st.cache_resource
def get_embeddings():
    """Initialize embeddings with latest langchain-huggingface"""
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Test embeddings
        test_embedding = embeddings.embed_query("test")
        if len(test_embedding) > 0:
            return embeddings
        else:
            return None
            
    except Exception as e:
        return None

# Enhanced caching system
def get_file_hash(filepath_or_content) -> str:
    """Generate hash from file path or content"""
    if isinstance(filepath_or_content, (str, Path)) and os.path.exists(filepath_or_content):
        # File path - read and hash
        with open(filepath_or_content, 'rb') as f:
            content = f.read()
    else:
        # Direct content
        content = filepath_or_content
        if isinstance(content, str):
            content = content.encode('utf-8')
    
    return hashlib.md5(content).hexdigest()

def get_cache_key(local_files: List[str], uploaded_files: List) -> str:
    """Generate cache key from all files"""
    file_hashes = {}
    
    # Hash local files
    for filepath in local_files:
        try:
            file_hashes[f"local_{filepath}"] = get_file_hash(filepath)
        except:
            continue
    
    # Hash uploaded files
    for uploaded_file in uploaded_files:
        try:
            file_hashes[f"upload_{uploaded_file.name}"] = get_file_hash(uploaded_file.getvalue())
        except:
            continue
    
    cache_string = json.dumps(sorted(file_hashes.items()), sort_keys=True)
    return hashlib.md5(cache_string.encode()).hexdigest()[:16]

def save_vectors_to_cache(vectorstore: FAISS, cache_key: str, file_info: Dict):
    """Save vectorstore to cache"""
    try:
        cache_path = CACHE_DIR / f"vectors_{cache_key}"
        cache_path.mkdir(exist_ok=True)
        
        # Save FAISS index
        vectorstore.save_local(str(cache_path))
        
        # Save metadata
        metadata = {
            "file_info": file_info,
            "timestamp": time.time(),
            "cache_key": cache_key,
            "chunks": st.session_state.document_chunks
        }
        
        with open(cache_path / "metadata.json", "w", encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        return False

def load_vectors_from_cache(cache_key: str):
    """Load vectorstore from cache"""
    try:
        cache_path = CACHE_DIR / f"vectors_{cache_key}"
        metadata_path = cache_path / "metadata.json"
        
        if not cache_path.exists() or not metadata_path.exists():
            return None, None
        
        # Load metadata
        with open(metadata_path, "r", encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Check if cache is not too old (24 hours)
        if time.time() - metadata["timestamp"] > 86400:
            return None, None
        
        # Load embeddings
        embeddings = get_embeddings()
        if not embeddings:
            return None, None
        
        # Load FAISS vectorstore
        vectorstore = FAISS.load_local(
            str(cache_path), 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        
        return vectorstore, metadata
        
    except Exception as e:
        return None, None

# Enhanced document processing
def load_single_local_file(filepath: str) -> List[Document]:
    """Load a single local file"""
    try:
        file_extension = Path(filepath).suffix.lower()
        
        # Use appropriate loader based on file type
        if file_extension == '.pdf':
            loader = PyPDFLoader(filepath)
        elif file_extension == '.csv':
            loader = CSVLoader(filepath)
        elif file_extension == '.txt':
            loader = TextLoader(filepath, encoding='utf-8')
        elif file_extension in ['.xlsx', '.xls']:
            loader = UnstructuredExcelLoader(filepath)
        elif file_extension == '.docx':
            # Try to load docx if available
            try:
                from langchain_community.document_loaders import UnstructuredWordDocumentLoader
                loader = UnstructuredWordDocumentLoader(filepath)
            except:
                return []
        else:
            return []
        
        docs = loader.load()
        cleaned_docs = []
        
        for i, doc in enumerate(docs):
            if hasattr(doc, 'page_content') and doc.page_content.strip():
                content = doc.page_content.replace('\n\n', '\n').strip()
                if len(content) > 50:  # Only include meaningful content
                    doc.page_content = content
                    # Enhanced metadata
                    doc.metadata.update({
                        'source_file': os.path.basename(filepath),
                        'file_path': filepath,
                        'file_hash': get_file_hash(filepath),
                        'file_type': 'local',
                        'chunk_id': i,
                        'content_length': len(content)
                    })
                    cleaned_docs.append(doc)
        
        return cleaned_docs
        
    except Exception as e:
        return []

def load_single_uploaded_file(uploaded_file) -> List[Document]:
    """Load a single uploaded file"""
    try:
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_file_path = temp_file.name
        
        try:
            # Updated document loaders
            if file_extension == 'pdf':
                loader = PyPDFLoader(temp_file_path)
            elif file_extension == 'csv':
                loader = CSVLoader(temp_file_path)
            elif file_extension == 'txt':
                loader = TextLoader(temp_file_path, encoding='utf-8')
            elif file_extension in ['xlsx', 'xls']:
                loader = UnstructuredExcelLoader(temp_file_path)
            else:
                return []
            
            docs = loader.load()
            cleaned_docs = []
            
            for i, doc in enumerate(docs):
                if hasattr(doc, 'page_content') and doc.page_content.strip():
                    content = doc.page_content.replace('\n\n', '\n').strip()
                    if len(content) > 50:  # Only include meaningful content
                        doc.page_content = content
                        # Enhanced metadata
                        doc.metadata.update({
                            'source_file': uploaded_file.name,
                            'file_hash': get_file_hash(uploaded_file.getvalue()),
                            'file_type': 'uploaded',
                            'upload_time': time.time(),
                            'file_size': len(uploaded_file.getvalue()),
                            'chunk_id': i,
                            'content_length': len(content)
                        })
                        cleaned_docs.append(doc)
            
            return cleaned_docs
            
        finally:
            try:
                os.unlink(temp_file_path)
            except:
                pass
        
    except Exception as e:
        return []

def process_all_documents(local_files: List[str], uploaded_files: List, show_progress: bool = True) -> bool:
    """Process all documents with intelligent caching"""
    t = translations[st.session_state.language]
    
    total_files = len(local_files) + len(uploaded_files)
    if total_files == 0:
        return False
    
    # Generate cache key
    cache_key = get_cache_key(local_files, uploaded_files)
    
    # Try to load from cache
    if show_progress and st.session_state.show_loading_messages:
        cache_status = st.empty()
        cache_status.info(f"üîç {t['checking_cache']}")
    
    cached_vectorstore, cached_metadata = load_vectors_from_cache(cache_key)
    
    if cached_vectorstore and cached_metadata:
        if show_progress and st.session_state.show_loading_messages:
            cache_status.success(f"‚úÖ {t['found_cached']}")
            time.sleep(0.5)
            cache_status.empty()
        
        st.session_state.vectorstore = cached_vectorstore
        st.session_state.document_chunks = cached_metadata.get("chunks", 0)
        st.session_state.documents_processed = True
        return True
    
    if show_progress and st.session_state.show_loading_messages:
        cache_status.empty()
    
    # Process documents if not in cache
    if show_progress and st.session_state.show_loading_messages:
        process_status = st.empty()
        process_status.info(f"üìù {t['processing']}")
    
    all_documents = []
    progress_bar = st.progress(0) if show_progress else None
    status_text = st.empty() if show_progress else None
    
    try:
        total_files_to_process = len(local_files) + len(uploaded_files)
        processed = 0
        
        # Process local files
        for filepath in local_files:
            if show_progress and status_text:
                status_text.text(f"Loading local: {os.path.basename(filepath)}...")
            if progress_bar:
                progress_bar.progress(processed / total_files_to_process * 0.5)
            
            docs = load_single_local_file(filepath)
            all_documents.extend(docs)
            processed += 1
        
        # Process uploaded files
        for uploaded_file in uploaded_files:
            if show_progress and status_text:
                status_text.text(f"Loading uploaded: {uploaded_file.name}...")
            if progress_bar:
                progress_bar.progress(processed / total_files_to_process * 0.5)
            
            docs = load_single_uploaded_file(uploaded_file)
            all_documents.extend(docs)
            processed += 1
        
        if not all_documents:
            return False
        
        # Split documents with updated text splitter
        if show_progress and status_text:
            status_text.text("üîÑ Splitting documents...")
        if progress_bar:
            progress_bar.progress(0.7)
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=50,
            separators=["\n\n", "\n", ". ", " ", ""],
            length_function=len,
        )
        texts = text_splitter.split_documents(all_documents)
        
        if not texts:
            return False
        
        # Create embeddings and vectorstore
        if show_progress and status_text:
            status_text.text("üìä Creating embeddings...")
        if progress_bar:
            progress_bar.progress(0.9)
        
        embeddings = get_embeddings()
        if not embeddings:
            return False
        
        vectorstore = FAISS.from_documents(texts, embeddings)
        
        # Save to cache
        if show_progress and status_text:
            status_text.text(f"üíæ {t['saving_cache']}")
        file_info = {
            "local_files": local_files,
            "uploaded_files": [f.name for f in uploaded_files],
            "total_documents": len(all_documents),
            "total_chunks": len(texts)
        }
        save_vectors_to_cache(vectorstore, cache_key, file_info)
        
        # Update session state
        st.session_state.vectorstore = vectorstore
        st.session_state.document_chunks = len(texts)
        st.session_state.documents_processed = True
        
        if progress_bar:
            progress_bar.progress(1.0)
            time.sleep(0.3)
            progress_bar.empty()
        if status_text:
            status_text.empty()
        if show_progress and st.session_state.show_loading_messages:
            process_status.empty()
        
        return True
        
    except Exception as e:
        if progress_bar:
            progress_bar.empty()
        if status_text:
            status_text.empty()
        if show_progress and st.session_state.show_loading_messages:
            process_status.empty()
        return False

# Auto-load local files on startup with improved UX
def auto_load_local_files():
    """Automatically load local files on startup with better UX"""
    app_state = get_app_state()
    
    # Check if this is a fresh app start (not just a user session)
    if app_state.get("initialized", False) and (time.time() - app_state.get("last_load", 0)) < 300:  # 5 minutes
        # App was recently initialized, skip auto-load messages
        st.session_state.show_loading_messages = False
    
    if st.session_state.auto_load_attempted:
        return
    
    st.session_state.auto_load_attempted = True
    
    # Scan for local files
    local_files = scan_local_files()
    st.session_state.local_files = local_files
    
    t = translations[st.session_state.language]
    
    if local_files:
        # Show initial file discovery message only once per app session
        if st.session_state.show_loading_messages:
            discovery_msg = st.empty()
            discovery_msg.success(t["found_local_files"](len(local_files)))
            time.sleep(1)
        
        # Auto-process if we have local files
        with st.spinner(t["processing_local"]) if st.session_state.show_loading_messages else st.empty():
            success = process_all_documents(local_files, [], show_progress=st.session_state.show_loading_messages)
            
            if success:
                if st.session_state.show_loading_messages:
                    completion_msg = st.empty()
                    completion_msg.success(f"‚úÖ {t['system_ready']}")
                    time.sleep(1)
                    completion_msg.empty()
                    discovery_msg.empty()
                
                # Update app state to indicate successful initialization
                app_state["initialized"] = True
                app_state["last_load"] = time.time()
                save_app_state(app_state)
                
                # Disable loading messages for subsequent users
                st.session_state.show_loading_messages = False
                st.session_state.initialization_complete = True

# Enhanced query processing
def setup_advanced_retrieval_chain():
    """Setup retrieval chain with advanced features"""
    try:
        if not st.session_state.vectorstore:
            return None
        
        # Create retriever with similarity threshold
        retriever = st.session_state.vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 5,
                "score_threshold": st.session_state.similarity_threshold
            }
        )
        
        # Enhanced memory
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # Updated Gemini model with latest API
        llm = ChatGoogleGenerativeAI(
            model="gemini-pro",
            temperature=st.session_state.temperature,
            max_tokens=st.session_state.max_tokens,
            google_api_key=GOOGLE_API_KEY,
        )
        
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=False
        )
        
        return qa_chain
        
    except Exception as e:
        return None

def query_with_analytics(chain, question: str) -> Dict[str, Any]:
    """Query with analytics and error handling"""
    try:
        start_time = time.time()
        
        # Store search history
        st.session_state.search_history.append({
            "question": question,
            "timestamp": time.time()
        })
        
        # Keep only last 10 searches
        if len(st.session_state.search_history) > 10:
            st.session_state.search_history = st.session_state.search_history[-10:]
        
        response = chain.invoke({"question": question})
        
        # Add analytics
        response["query_time"] = time.time() - start_time
        response["question"] = question
        
        return response
        
    except Exception as e:
        error_msg = str(e).lower()
        if "quota" in error_msg or "rate" in error_msg or "429" in error_msg:
            raise Exception("RATE_LIMIT")
        elif "timeout" in error_msg:
            raise TimeoutError("Request timed out")
        else:
            raise e

# UI Helper functions
def display_file_lists():
    """Display local and uploaded file lists"""
    t = translations[st.session_state.language]
    
    # Local files
    if st.session_state.local_files:
        st.markdown(f"**üìÅ {t['local_files']} ({len(st.session_state.local_files)})**")
        for filepath in st.session_state.local_files[:5]:  # Show first 5
            filename = os.path.basename(filepath)
            file_size = ""
            try:
                size_kb = os.path.getsize(filepath) / 1024
                file_size = f" ({size_kb:.1f}KB)"
            except:
                pass
            st.text(f"üìÑ {filename}{file_size}")
        
        if len(st.session_state.local_files) > 5:
            st.text(f"... and {len(st.session_state.local_files) - 5} more files")
    
    # Uploaded files
    if st.session_state.uploaded_files:
        st.markdown(f"**üì§ {t['uploaded_files']} ({len(st.session_state.uploaded_files)})**")
        for uploaded_file in st.session_state.uploaded_files:
            file_size = len(uploaded_file.getvalue()) / 1024
            st.text(f"üìÑ {uploaded_file.name} ({file_size:.1f}KB)")

def display_advanced_settings():
    """Display advanced settings"""
    with st.expander("‚öôÔ∏è Advanced Settings"):
        st.session_state.similarity_threshold = st.slider(
            "Similarity Threshold",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.similarity_threshold,
            step=0.1,
            help="Higher values = more strict matching"
        )
        
        st.session_state.max_tokens = st.slider(
            "Max Response Tokens",
            min_value=128,
            max_value=2048,
            value=st.session_state.max_tokens,
            step=128
        )
        
        st.session_state.temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.temperature,
            step=0.1,
            help="Higher values = more creative responses"
        )

def cleanup_cache():
    """Clean up old cache files"""
    try:
        current_time = time.time()
        for cache_dir in CACHE_DIR.glob("vectors_*"):
            metadata_path = cache_dir / "metadata.json"
            if metadata_path.exists():
                try:
                    with open(metadata_path, "r") as f:
                        metadata = json.load(f)
                    
                    # Delete if older than 7 days
                    if current_time - metadata["timestamp"] > 604800:
                        import shutil
                        shutil.rmtree(cache_dir)
                except:
                    continue
    except Exception as e:
        pass

# Main application
def main():
    try:
        # Handle language
        if "language" in st.query_params:
            st.session_state.language = st.query_params["language"]

        t = translations[st.session_state.language]

        # Enhanced CSS with better styling
        st.markdown("""
            <style>
            .main .block-container {
                max-width: 1200px;
                padding-top: 4rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏ö navbar */
                padding-bottom: 8rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô */
            }
            @media (max-width: 768px) {
                .main .block-container {
                    padding-top: 3.5rem; /* ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠ */
                    padding-bottom: 10rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠ */
                    padding-left: 1rem;
                    padding-right: 1rem;
                }
            }
            .stTitle {
                text-align: center;
                color: #1f77b4;
                margin-top: 1.5rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏° margin-top ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏ö navbar */
                margin-bottom: 2rem;
                font-size: 2.5rem !important;
                font-weight: 700;
                line-height: 1.4 !important;
                padding: 1rem 0; /* ‡πÄ‡∏û‡∏¥‡πà‡∏° padding ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡πà‡∏≤‡∏á */
                display: flex;
                align-items: center;
                justify-content: center;
                min-height: 5rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ */
                position: relative;
                z-index: 10; /* ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ element ‡∏≠‡∏∑‡πà‡∏ô */
            }
            .title-emoji {
                font-size: 3.2rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ */
                margin-right: 0.6rem;
                filter: none;
                background: none;
                line-height: 1;
                display: inline-block;
                vertical-align: middle;
                padding: 0.3rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏° padding */
                margin-top: -0.2rem; /* ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á */
            }
            .title-text {
                background: linear-gradient(90deg, #1f77b4, #2ca02c);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                background-clip: text;
                font-weight: 700;
                line-height: 1.3; /* ‡∏õ‡∏£‡∏±‡∏ö line-height */
                display: inline-block;
                vertical-align: middle;
                padding: 0.2rem 0; /* ‡πÄ‡∏û‡∏¥‡πà‡∏° padding */
            }
            .metric-card {
                background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
                border: 1px solid #dee2e6;
                border-radius: 12px;
                padding: 1.2rem;
                margin: 0.5rem 0;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            .model-info {
                background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
                border-left: 4px solid #2196f3;
                padding: 1rem;
                border-radius: 8px;
                margin: 1rem 0;
                font-size: 0.95rem;
                line-height: 1.6;
            }
            .model-info .emoji {
                font-size: 1.2rem;
                margin-right: 0.3rem;
                filter: none;
                background: none;
                line-height: 1;
                display: inline-block;
                vertical-align: middle;
                padding: 0.1rem;
            }
            .model-info .bold-text {
                font-weight: 700;
                color: #1976d2;
                vertical-align: middle;
            }
            .chat-container {
                background: #fafafa;
                border-radius: 12px;
                padding: 1rem;
                margin-top: 1rem;
                margin-bottom: 6rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô */
                min-height: 50vh;
            }
            @media (max-width: 768px) {
                .chat-container {
                    margin-bottom: 8rem; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠ */
                    padding: 0.8rem;
                }
            }
            .welcome-card {
                background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
                border: 2px solid #ff9800;
                border-radius: 16px;
                padding: 2rem;
                margin: 2rem 0 6rem 0; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô */
                text-align: center;
                box-shadow: 0 4px 12px rgba(255, 152, 0, 0.2);
            }
            @media (max-width: 768px) {
                .welcome-card {
                    padding: 1.5rem;
                    margin: 1rem 0 8rem 0; /* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠ */
                }
            }
            .status-badge {
                display: inline-block;
                padding: 0.3rem 0.8rem;
                border-radius: 20px;
                font-size: 0.8rem;
                font-weight: 600;
                margin: 0.2rem;
            }
            .status-ready {
                background-color: #d4edda;
                color: #155724;
                border: 1px solid #c3e6cb;
            }
            .footer {
                position: fixed;
                left: 0;
                bottom: 0;
                width: 100%;
                text-align: center;
                padding: 8px 0;
                font-size: 12px;
                color: #545454;
                background-color: rgba(255, 255, 255, 0.95);
                backdrop-filter: blur(10px);
                border-top: 1px solid #eee;
                z-index: 999;
                font-weight: 500;
                box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
            }
            @media (max-width: 768px) {
                .footer {
                    font-size: 10px;
                    padding: 6px 0;
                    line-height: 1.2;
                }
            }
            @media (max-width: 480px) {
                .footer {
                    font-size: 9px;
                    padding: 4px 0;
                }
            }
            </style>
        """, unsafe_allow_html=True)

        # Title first - prominent display
        st.markdown(f'''
            <div class="stTitle">
                <span class="title-emoji">ü§ñ</span>
                <span class="title-text">{t["title"].replace("ü§ñ ", "")}</span>
            </div>
        ''', unsafe_allow_html=True)

        if not st.session_state.app_initialized:
            st.session_state.app_initialized = True
            cleanup_cache()

        # Auto-load local files on first run with improved UX
        auto_load_local_files()

        # Model info with better styling
        st.markdown(f'<div class="model-info">{t["model_info"]}</div>', unsafe_allow_html=True)

        # Sidebar
        with st.sidebar:
            # Language selection
            st.markdown(f'<div class="emoji-text"><span class="emoji-inline">üåê</span><span class="bold-text">{t["language"].replace("üåê ", "")}</span></div>', unsafe_allow_html=True)
            selected_lang = st.selectbox(
                "Select Language",
                options=["‡πÑ‡∏ó‡∏¢", "English"],
                index=1 if st.session_state.language == "en" else 0,
                label_visibility="collapsed"
            )
            
            new_language = "th" if selected_lang == "‡πÑ‡∏ó‡∏¢" else "en"
            if new_language != st.session_state.language:
                st.session_state.language = new_language

            st.markdown("---")

            # Debug mode
            st.session_state.debug_mode = st.checkbox("üîç Debug Mode")

            # Reload local files button
            if st.button(t["reload_local"], use_container_width=True):
                st.session_state.local_files = scan_local_files()
                st.session_state.auto_load_attempted = False
                st.session_state.documents_processed = False
                st.session_state.vectorstore = None
                st.session_state.show_loading_messages = True
                # Reset app state to allow messages to show again
                save_app_state({"initialized": False, "last_load": 0})
                st.rerun()

            # File uploader
            st.markdown(f'<div class="emoji-text"><span class="emoji-inline">üì§</span><span class="bold-text">{t["upload_button"]}</span></div>', unsafe_allow_html=True)
            uploaded_files = st.file_uploader(
                "Upload Additional Documents",
                accept_multiple_files=True,
                type=['pdf', 'csv', 'txt', 'xlsx', 'xls'],
                label_visibility="collapsed"
            )

            # Process additional uploaded files
            if uploaded_files and uploaded_files != st.session_state.uploaded_files:
                st.session_state.uploaded_files = uploaded_files
                with st.spinner(t["processing"]):
                    success = process_all_documents(st.session_state.local_files, uploaded_files, show_progress=True)
                    if success:
                        st.success(t["upload_success"](len(uploaded_files)))
                    else:
                        st.error(t["error_processing"])

            st.markdown("---")

            # Display file lists
            display_file_lists()

            # Advanced settings
            display_advanced_settings()

            # Statistics
            if st.session_state.debug_mode and st.session_state.documents_processed:
                st.markdown(f'<div class="emoji-text"><span class="emoji-inline">üìä</span><span class="bold-text">{t["stats"]}</span></div>', unsafe_allow_html=True)
                st.write(f"üìÅ Local files: {len(st.session_state.local_files)}")
                st.write(f"üì§ Uploaded: {len(st.session_state.uploaded_files)}")
                st.write(f"üî¢ Chunks: {st.session_state.document_chunks}")
                st.write(f"üîç Searches: {len(st.session_state.search_history)}")
                
                cache_files = list(CACHE_DIR.glob("vectors_*"))
                st.write(f"üíæ Cache files: {len(cache_files)}")

            # Clear buttons - ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡πÅ‡∏ñ‡∏ß
            if st.button(t["clear_chat"], use_container_width=True):
                st.session_state.messages = []
                st.success("‚úÖ Chat cleared!")
            
            if st.button(t["clear_cache"], use_container_width=True):
                try:
                    import shutil
                    if CACHE_DIR.exists():
                        shutil.rmtree(CACHE_DIR)
                        CACHE_DIR.mkdir(exist_ok=True)
                    st.session_state.vectorstore = None
                    st.session_state.documents_processed = False
                    st.session_state.auto_load_attempted = False
                    st.session_state.show_loading_messages = True
                    # Reset app state
                    save_app_state({"initialized": False, "last_load": 0})
                    st.success("‚úÖ Cache cleared!")
                except Exception as e:
                    st.error(f"Error clearing cache: {e}")

        # Main content
        if st.session_state.documents_processed:
            # Status indicator
            if st.session_state.initialization_complete:
                st.markdown(f'<span class="status-badge status-ready">‚úÖ {t["system_ready"]}</span>', unsafe_allow_html=True)
                st.markdown("<br>", unsafe_allow_html=True)

            # Chat interface in container
            st.markdown('<div class="chat-container">', unsafe_allow_html=True)
            
            # Display chat messages
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

            # Chat input
            if prompt := st.chat_input(t["ask_placeholder"]):
                # Add user message
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                # Generate response
                with st.chat_message("assistant"):
                    retrieval_chain = setup_advanced_retrieval_chain()
                    if retrieval_chain:
                        with st.spinner(t["thinking"]):
                            try:
                                response = query_with_analytics(retrieval_chain, prompt)
                                answer = response.get('answer', 'No answer generated')
                                
                                st.markdown(answer)
                                st.session_state.messages.append({"role": "assistant", "content": answer})
                                
                                # Enhanced source display
                                if 'source_documents' in response and response['source_documents']:
                                    with st.expander(f"üìö Sources ({len(response['source_documents'])})"):
                                        for i, doc in enumerate(response['source_documents']):
                                            st.markdown(f'<span class="bold-text">Source {i+1}:</span>', unsafe_allow_html=True)
                                            content = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                                            st.markdown(content)
                                            
                                            if hasattr(doc, 'metadata') and doc.metadata:
                                                meta = doc.metadata
                                                source_type = "üìÅ Local" if meta.get('file_type') == 'local' else "üì§ Uploaded"
                                                st.caption(f"{source_type}: {meta.get('source_file', 'Unknown')}")
                                                if st.session_state.debug_mode:
                                                    st.caption(f"üî¢ Chunk: {meta.get('chunk_id', 'N/A')} | Length: {meta.get('content_length', 'N/A')}")
                                            st.markdown("---")
                                
                                # Query analytics
                                if st.session_state.debug_mode:
                                    st.caption(f"‚è±Ô∏è Query time: {response.get('query_time', 0):.2f}s")
                                
                            except TimeoutError:
                                st.error("‚è±Ô∏è Request timed out. Please try a shorter question.")
                            except Exception as e:
                                if "RATE_LIMIT" in str(e):
                                    st.error("‚è≥ API rate limit reached. Please wait and try again.")
                                else:
                                    st.error(f"üö® Error: {str(e)}")
                    else:
                        st.error("‚ö†Ô∏è Could not set up the retrieval system.")
            
            st.markdown('</div>', unsafe_allow_html=True)

        else:
            # Welcome message with enhanced styling
            st.markdown(f"""
                <div class="welcome-card">
                    <h3>üëã {t['welcome']}</h3>
                    <p>System is ready for document-based conversations!</p>
                </div>
            """, unsafe_allow_html=True)
            
            with st.expander("‚ÑπÔ∏è How to use / ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", expanded=True):
                if st.session_state.language == "en":
                    st.markdown("""
                    <div class="bold-text">üöÄ Advanced RAG Chatbot with Auto-Load:</div>
                    
                    <div class="bold-text">üìÅ Auto-Detection:</div>
                    - Automatically scans repository for PDF, TXT, CSV, XLSX files
                    - Respects .gitignore patterns
                    - Loads documents on startup
                    
                    <div class="bold-text">üì§ Additional Upload:</div>
                    - Upload more documents using the sidebar
                    - Combines with auto-detected files
                    - Smart caching for fast reloads
                    
                    <div class="bold-text">ü§ñ AI Features:</div>
                    - Gemini Pro language model
                    - MiniLM-L6-v2 embeddings for semantic search
                    - Adjustable similarity threshold
                    - Configurable response parameters
                    
                    <div class="bold-text">üíæ Smart Caching:</div>
                    - Automatic vector caching
                    - Fast reload for same documents
                    - Cache cleanup and management
                    - Persistent storage across sessions
                    
                    <div class="bold-text">üìä File Types Supported:</div>
                    - üìÑ PDF files
                    - üìù Text files (.txt)
                    - üìä CSV files
                    - üìà Excel files (.xlsx, .xls)
                    - üìÑ Word documents (.docx)
                    """, unsafe_allow_html=True)
                else:
                    st.markdown("""
                    <div class="bold-text">üöÄ Advanced RAG Chatbot ‡∏û‡∏£‡πâ‡∏≠‡∏° Auto-Load:</div>
                    
                    <div class="bold-text">üìÅ ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥:</div>
                    - ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå PDF, TXT, CSV, XLSX ‡πÉ‡∏ô repository ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
                    - ‡πÄ‡∏Ñ‡∏≤‡∏£‡∏û‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö .gitignore
                    - ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                    
                    <div class="bold-text">üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:</div>
                    - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ñ‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏á
                    - ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
                    - Smart caching ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏£‡πá‡∏ß
                    
                    <div class="bold-text">ü§ñ ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå AI:</div>
                    - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤ Gemini Pro
                    - MiniLM-L6-v2 embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
                    - ‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÑ‡∏î‡πâ
                    - ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ
                    
                    <div class="bold-text">üíæ Smart Caching:</div>
                    - Cache vectors ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
                    - ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏°
                    - ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ cache
                    - ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏≤‡∏ß‡∏£‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ã‡∏™‡∏ä‡∏±‡πà‡∏ô
                    
                    <div class="bold-text">üìä ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö:</div>
                    - üìÑ ‡πÑ‡∏ü‡∏•‡πå PDF
                    - üìù ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (.txt)
                    - üìä ‡πÑ‡∏ü‡∏•‡πå CSV
                    - üìà ‡πÑ‡∏ü‡∏•‡πå Excel (.xlsx, .xls)
                    - üìÑ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ Word (.docx)
                    """, unsafe_allow_html=True)

            total_files = len(st.session_state.local_files) + len(st.session_state.uploaded_files)
            if total_files == 0:
                st.info(t["no_documents"])
            else:
                st.info(f"üìÅ Found {len(st.session_state.local_files)} local files. Ready to chat!")

        # Footer with mobile-responsive text
        # footer_text = "ü§ñ Gen AI : RAG Chatbot with Documents (Demo) | Enhanced UX/UI"
        # if st.query_params.get("mobile", "false") == "true":
        #     footer_text = "ü§ñ Gen AI : RAG Chatbot with Documents (Demo)"
        
        # st.markdown(
        #     f'<div class="footer">{footer_text}<br><small>Created by Arnutt Noitumyae, 2024</small></div>',
        #     unsafe_allow_html=True
        # )
        
    except Exception as e:
        st.error(f"Application error: {str(e)}")
        if st.session_state.debug_mode:
            st.code(str(e))

if __name__ == "__main__":
    main()
